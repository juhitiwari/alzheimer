{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitt cookie audio extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pylangacq as pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .cha paths\n",
    "pitt_path = Path('../data/Pitt/')\n",
    "control_path = pitt_path/'Control'/'cookie'\n",
    "dementia_path = pitt_path/'Dementia'/'cookie'\n",
    "\n",
    "# media paths\n",
    "media_path = Path('../data/media-cookie/Pitt/cookie')\n",
    "media_control_path = media_path/'Control'\n",
    "media_dementia_path = media_path/'Dementia'\n",
    "\n",
    "# Extracted output paths\n",
    "output_path = Path('../data/media-cookie/Pitt/cookie-par')\n",
    "utterances_path = output_path/'utterances'\n",
    "utterances_control_path = utterances_path/'Control'\n",
    "utterances_dementia_path = utterances_path/'Dementia'\n",
    "descriptions_path = output_path/'descriptions'\n",
    "descriptions_control_path = descriptions_path/'Control'\n",
    "descriptions_dementia_path = descriptions_path/'Dementia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3767, 7949),\n",
       " (8972, 11606),\n",
       " (12100, 13307),\n",
       " (20247, 25102),\n",
       " (25102, 26749),\n",
       " (27446, 34019)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraction of time marks from 'PAR' participants in .cha files\n",
    "# The time marks are the last part of the last line\n",
    "# of the participant's transcription.\n",
    "def extract_times_from_file(file):\n",
    "    par_re = re.compile(r'^\\*PAR:\\s(.*)')\n",
    "    cont_re = re.compile(r'^\\t(.*)')\n",
    "    time_re = re.compile('.*\\x15(\\d+)_(\\d+)\\x15$')\n",
    "    \n",
    "    document = open(file).read()\n",
    "    doc = document.split('\\n')\n",
    "\n",
    "    time_list = []\n",
    "    in_par = False\n",
    "    for line in doc:\n",
    "        pattern = cont_re if in_par else par_re\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            in_par = True\n",
    "            utterance = match.group(1)\n",
    "            time_match = time_re.match(utterance)\n",
    "            if time_match:\n",
    "                time_begin = int(time_match.group(1))\n",
    "                time_end = int(time_match.group(2))\n",
    "                time_list.append((time_begin, time_end))\n",
    "        else:\n",
    "            in_par = False\n",
    "            \n",
    "    return(time_list)\n",
    "\n",
    "times = extract_times_from_file(dementia_path/'703-0.cha')\n",
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like those numbers must be milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all utterances and complete patient descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take the opportunity to downsample and convert to mono to avoid doing it during training. We'll keep the source signal characteristics for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(source_cha_path, source_audio_path, utterances_output, descriptions_output):    \n",
    "    files = sorted(glob.glob(str(source_cha_path)))\n",
    "    for file in files:       \n",
    "        filepath = Path(file)\n",
    "        audio = (source_audio_path/(filepath.stem)).with_suffix('.mp3')\n",
    "        print(\"Processing\", audio)\n",
    "        \n",
    "        times = extract_times_from_file(file)\n",
    "        try:\n",
    "            (signal, sr) = torchaudio.load(audio)\n",
    "        except:\n",
    "            print(\"Could not open file\", audio)\n",
    "            continue\n",
    "        \n",
    "        description = None\n",
    "        for i, (t_begin, t_end) in enumerate(times):\n",
    "            s_begin = round(t_begin * sr / 1000)\n",
    "            s_end = round(t_end * sr / 1000)\n",
    "            utterance = signal[:, s_begin:s_end]\n",
    "            # Some annotations yield 0 samples (i.e., '066-0' in the dementia set)\n",
    "            # The values seem to be off in that file\n",
    "            # TODO: check others\n",
    "            if utterance.shape[1] == 0:\n",
    "                print(\"Warning: 0-size fragment\", i, audio)\n",
    "                continue\n",
    "            utterance_out = (utterances_output/(filepath.stem + f'-{i:02}')).with_suffix('.mp3')\n",
    "            torchaudio.save(str(utterance_out), utterance, sr)\n",
    "            \n",
    "            # Concat all participant utterances\n",
    "            description = utterance if description is None else torch.cat((description, utterance), 1)\n",
    "            \n",
    "        description_out = (descriptions_output/(filepath.stem)).with_suffix('.mp3')\n",
    "        torchaudio.save(str(description_out), description, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell extracts all the files\n",
    "extract_audio(control_path/'*.cha', media_control_path, utterances_control_path, descriptions_control_path)\n",
    "extract_audio(dementia_path/'*.cha', media_dementia_path, utterances_dementia_path, descriptions_dementia_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
