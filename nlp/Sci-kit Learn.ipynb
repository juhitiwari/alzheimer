{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterances por frase sin simbolos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pylangacq as pla\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eve = pla.read_chat('./Corpus/Pitt/Control/cookie/*.cha', encoding = 'utf-8')\n",
    "tagged = eve.tagged_sents(participant='PAR')\n",
    "sentences = eve.sents(participant='PAR')\n",
    "group = list(['control']*len(tagged))\n",
    "control_df = pd.DataFrame(\n",
    "    {'utterances': sentences,\n",
    "     'tagged': tagged,\n",
    "     'group': group\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eve = pla.read_chat('./Corpus/Pitt/Dementia/cookie/*.cha', encoding = 'utf-8')\n",
    "tagged = eve.tagged_sents(participant='PAR')\n",
    "sentences = eve.sents(participant='PAR')\n",
    "group = list(['dementia']*len(tagged))\n",
    "dementia_df = pd.DataFrame(\n",
    "    {'utterances': sentences,\n",
    "     'tagged': tagged,\n",
    "     'group': group\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterances</th>\n",
       "      <th>tagged</th>\n",
       "      <th>group</th>\n",
       "      <th>utterances_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[that's, about, all, .]</td>\n",
       "      <td>[(that's, PRO:DEM, that, (1, 2, SUBJ)), (CLITI...</td>\n",
       "      <td>dementia</td>\n",
       "      <td>that's about all .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what's, it, always, say, that, ?]</td>\n",
       "      <td>[(what's, PRO:INT, what, (1, 5, SUBJ)), (CLITI...</td>\n",
       "      <td>dementia</td>\n",
       "      <td>what's it always say that ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, don't, see, anything, else, happening, .]</td>\n",
       "      <td>[(I, PRO:SUB, I, (1, 4, SUBJ)), (don't, MOD, d...</td>\n",
       "      <td>control</td>\n",
       "      <td>I don't see anything else happening .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[and, the, mother, is, washing, the, dishes, b...</td>\n",
       "      <td>[(and, COORD, and, (1, 5, LINK)), (the, DET:AR...</td>\n",
       "      <td>control</td>\n",
       "      <td>and the mother is washing the dishes but she's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[a, boy, is, getting, a, cookie, out_of, a, co...</td>\n",
       "      <td>[(a, DET:ART, a, (1, 2, DET)), (boy, N, boy, (...</td>\n",
       "      <td>control</td>\n",
       "      <td>a boy is getting a cookie out_of a cookie jar ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          utterances  \\\n",
       "0                            [that's, about, all, .]   \n",
       "1                 [what's, it, always, say, that, ?]   \n",
       "2      [I, don't, see, anything, else, happening, .]   \n",
       "3  [and, the, mother, is, washing, the, dishes, b...   \n",
       "4  [a, boy, is, getting, a, cookie, out_of, a, co...   \n",
       "\n",
       "                                              tagged     group  \\\n",
       "0  [(that's, PRO:DEM, that, (1, 2, SUBJ)), (CLITI...  dementia   \n",
       "1  [(what's, PRO:INT, what, (1, 5, SUBJ)), (CLITI...  dementia   \n",
       "2  [(I, PRO:SUB, I, (1, 4, SUBJ)), (don't, MOD, d...   control   \n",
       "3  [(and, COORD, and, (1, 5, LINK)), (the, DET:AR...   control   \n",
       "4  [(a, DET:ART, a, (1, 2, DET)), (boy, N, boy, (...   control   \n",
       "\n",
       "                                      utterances_str  \n",
       "0                                 that's about all .  \n",
       "1                        what's it always say that ?  \n",
       "2              I don't see anything else happening .  \n",
       "3  and the mother is washing the dishes but she's...  \n",
       "4  a boy is getting a cookie out_of a cookie jar ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unimos los dos df , reorganizamos random y pasamos la colummna utterances de list -> str\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.concat([control_df, dementia_df])\n",
    "df = shuffle(df).reset_index(drop=True)\n",
    "\n",
    "utterances_str = []\n",
    "for i in range(len(df['utterances'])):\n",
    "    list1 = df['utterances'][i]\n",
    "    str1 = ' '.join(list1)\n",
    "    utterances_str.append(str1)\n",
    "    \n",
    "df['utterances_str'] = utterances_str\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>control</td>\n",
       "      <td>is that it ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dementia</td>\n",
       "      <td>and he's getting cookies .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>the water's coming out_of the faucet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dementia</td>\n",
       "      <td>she's doing the dishes .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dementia</td>\n",
       "      <td>and the woman is drying dishes .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                    text\n",
       "0   control                            is that it ?\n",
       "1  dementia              and he's getting cookies .\n",
       "2   control  the water's coming out_of the faucet .\n",
       "3  dementia                she's doing the dishes .\n",
       "4  dementia        and the woman is drying dishes ."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[['group', 'utterances_str']].rename(columns={\"group\":\"label\", \"utterances_str\":\"text\"})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dementia    3908\n",
       "control     3145\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "X = df['text']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4725, 1403)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4725, 1403)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[615 445]\n",
      " [346 922]]\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     control       0.64      0.58      0.61      1060\n",
      "    dementia       0.67      0.73      0.70      1268\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      2328\n",
      "   macro avg       0.66      0.65      0.65      2328\n",
      "weighted avg       0.66      0.66      0.66      2328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6602233676975945\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterances por dialogo con simbolos con spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_cha = './Corpus/Pitt/Control/cookie/*.cha'\n",
    "#path_cha = './Corpus/Pitt/Dementia/cookie/*.cha'\n",
    "\n",
    "processed = []\n",
    "def process_with_symbols(path_cha):\n",
    "    import glob\n",
    "    import re\n",
    "    \n",
    "    files = sorted(glob.glob(path_cha))\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for file in files:\n",
    "        document = open(file).read()\n",
    "        doc = document.replace('\\t', ' ').split('\\n')\n",
    "        \n",
    "        \n",
    "        pre_list = []\n",
    "        for d in doc:\n",
    "            match = re.findall(r'\\*PAR:(.*)',d)\n",
    "            if (len(match) != 0):\n",
    "                pre_list.append(match)\n",
    "\n",
    "        listt = []\n",
    "        for p in pre_list:\n",
    "            match = re.sub('x15.*?x15', '', str(p))\n",
    "            match = re.sub('[\\\\\\]', '', match)\n",
    "            match = re.sub('[\"\"\"]', '', match)\n",
    "            match = re.sub('[\\\\\\']', '', match)\n",
    "            match = re.search(r'.*?\\[(.*)].*', match).group(1)\n",
    "            listt.append(match)\n",
    "        \n",
    "        processed.append(listt)\n",
    "    \n",
    "    return(processed)\n",
    "    \n",
    "\n",
    "utterances_symbol = process_with_symbols(path_cha) \n",
    "utterances_symbol_control = utterances_symbol\n",
    "len(utterances_symbol_control)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path_cha = './Corpus/Pitt/Control/cookie/*.cha'\n",
    "path_cha = './Corpus/Pitt/Dementia/cookie/*.cha'\n",
    "\n",
    "processed = []\n",
    "def process_with_symbols(path_cha):\n",
    "    import glob\n",
    "    import re\n",
    "    \n",
    "    files = sorted(glob.glob(path_cha))\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for file in files:\n",
    "        document = open(file).read()\n",
    "        doc = document.replace('\\t', ' ').split('\\n')\n",
    "        \n",
    "        \n",
    "        pre_list = []\n",
    "        for d in doc:\n",
    "            match = re.findall(r'\\*PAR:(.*)',d)\n",
    "            if (len(match) != 0):\n",
    "                pre_list.append(match)\n",
    "\n",
    "        listt = []\n",
    "        for p in pre_list:\n",
    "            match = re.sub('x15.*?x15', '', str(p))\n",
    "            match = re.sub('[\\\\\\]', '', match)\n",
    "            match = re.sub('[\"\"\"]', '', match)\n",
    "            match = re.sub('[\\\\\\']', '', match)\n",
    "            match = re.search(r'.*?\\[(.*)].*', match).group(1)\n",
    "            listt.append(match)\n",
    "        \n",
    "        processed.append(listt)\n",
    "    \n",
    "    return(processed)\n",
    "    \n",
    "\n",
    "utterances_symbol = process_with_symbols(path_cha)    \n",
    "\n",
    "\n",
    "utterances_symbol_dementia = utterances_symbol\n",
    "len(utterances_symbol_dementia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = list([0]*len(utterances_symbol_control)) # Control = 0\n",
    "control_df = pd.DataFrame(\n",
    "    {'label': group,\n",
    "     'text': utterances_symbol_control\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = list([1]*len(utterances_symbol_dementia)) # Dementia = 1\n",
    "dementia_df = pd.DataFrame(\n",
    "    {'label': group,\n",
    "     'text': utterances_symbol_dementia\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[ well the [/] &amp;uh the little boy is (.) steal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[ okay mother is drying the dishes but the wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[ (.) what ? [+ exc] ,  the boys &amp;uh handing &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[ the boy is &amp;uh tipping stool reaching for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[ well the kids &amp;uh stealin(g) cookies out o(f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[ things that are happening . [+ exc] ,  girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[ this is &amp;uh a clause copy [/] copy c@l by hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>[ well theres a little boy &lt;on the&gt; [//] on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[ the sink is running over . ,  and &amp;uh the [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[ now wait a minute . [+ exc] ,  everything th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>[ stool &lt;is being&gt; [//] &amp;uh is &amp;k falling over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[ okay the waters running out_of the sink over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>[ yeah, alright . [+ exc] ,  the girl is reach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[ well the boy is getting cookies and hes gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>[ well I see the mother washing dishes . ,  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>[ &amp;uh &amp;th these two little &amp;g kids are gettin(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[ you mean starting from say left to right or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[ well hes into the cookie jar and the [/] the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[ okay start ? [+ exc] ,  okay a little boy is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[ &amp;uh &amp;ha theres a little boys takin(g) a cook...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0       1  [ well the [/] &uh the little boy is (.) steal...\n",
       "1       0  [ okay mother is drying the dishes but the wat...\n",
       "2       1  [ (.) what ? [+ exc] ,  the boys &uh handing &...\n",
       "3       0  [ the boy is &uh tipping stool reaching for th...\n",
       "4       0  [ well the kids &uh stealin(g) cookies out o(f...\n",
       "5       0  [ things that are happening . [+ exc] ,  girls...\n",
       "6       1  [ this is &uh a clause copy [/] copy c@l by hi...\n",
       "7       1  [ well theres a little boy <on the> [//] on a ...\n",
       "8       1  [ the sink is running over . ,  and &uh the [/...\n",
       "9       1  [ now wait a minute . [+ exc] ,  everything th...\n",
       "10      1  [ stool <is being> [//] &uh is &k falling over...\n",
       "11      0  [ okay the waters running out_of the sink over...\n",
       "12      1  [ yeah, alright . [+ exc] ,  the girl is reach...\n",
       "13      0  [ well the boy is getting cookies and hes gonn...\n",
       "14      1  [ well I see the mother washing dishes . ,  I ...\n",
       "15      1  [ &uh &th these two little &g kids are gettin(...\n",
       "16      0  [ you mean starting from say left to right or ...\n",
       "17      1  [ well hes into the cookie jar and the [/] the...\n",
       "18      0  [ okay start ? [+ exc] ,  okay a little boy is...\n",
       "19      0  [ &uh &ha theres a little boys takin(g) a cook..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unimos los dos df , reorganizamos random y pasamos la colummna utterances de list -> str\n",
    "df_spacy = pd.concat([control_df, dementia_df])\n",
    "df_spacy = shuffle(df_spacy).reset_index(drop=True)\n",
    "df_spacy.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_spacy.copy()\n",
    "text = df['text']\n",
    "\n",
    "to_df = []\n",
    "\n",
    "\n",
    "for i in text:\n",
    "    to_df.append(' '.join(i))\n",
    "\n",
    "df['text'] = to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' well the [/] &uh the little boy is (.) stealing cookies and  hes &uh partially gonna fall off the stool .   his sister is tellin(g) him to be quiet .   the mother is washing dishes .   an(d) &uh the water is overflowing onto the floor and shes'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = df['text']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 1702)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 1702)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "                    ])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 12]\n",
      " [16 45]]\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73        50\n",
      "           1       0.79      0.74      0.76        61\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       111\n",
      "   macro avg       0.75      0.75      0.75       111\n",
      "weighted avg       0.75      0.75      0.75       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 6 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    2.2s finished\n",
      "/home/gsus/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__max_df': [1.9], 'vect__max_features': [1000], 'vect__min_df': [50], 'vect__ngram_range': [(1, 1)], 'cls__C': (0.001, 0.0015, 0.01, 1, 10, 100), 'cls__loss': ['squared_hinge'], 'cls__max_iter': [1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = LinearSVC()\n",
    "#clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "#text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                    # ('clf', LinearSVC()),\n",
    "                   # ])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "               \n",
    "        \n",
    "# Feed the training data through the pipeline\n",
    "#text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#LinearSVC() es el clasificador\n",
    "\n",
    "pipeline = Pipeline(steps=[('vect', vectorizer), # Imposible poner más de 1 núcleo con CountVectorizer. Pra optimizar los hiperparámetros\n",
    "                           ('cls', LinearSVC()), # Primero he tendio que optimizar el modelo por un lado y por otro el vectorizador. Si no, era imposible.\n",
    "                    ])\n",
    " \n",
    "#pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                   #  ('clf', LinearSVC()),\n",
    "                   # ])\n",
    "\n",
    "#Aqui definimos el espacio de parámetros a explorar\n",
    "parameters = {'vect__max_df': [1.9],\n",
    "              'vect__max_features': [1000],\n",
    "              'vect__min_df': [50],\n",
    "              'vect__ngram_range': [(1, 1)],\n",
    "              'cls__C': (0.001, 0.0015, 0.01, 1, 10, 100),\n",
    "              'cls__loss': ['squared_hinge'],\n",
    "              'cls__max_iter': [1000],\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring='roc_auc',\n",
    "                           verbose=3,\n",
    "                           n_jobs = -1,\n",
    "                           cv=20                          \n",
    "                          )\n",
    "                         \n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__C': 0.0015,\n",
       " 'cls__loss': 'squared_hinge',\n",
       " 'cls__max_iter': 1000,\n",
       " 'vect__max_df': 1.9,\n",
       " 'vect__max_features': 1000,\n",
       " 'vect__min_df': 50,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.9, max_features=1000, min_df=50,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        stri...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid_search = grid_search.best_estimator_\n",
    "best_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 15]\n",
      " [ 9 52]]\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set\n",
    "predictions = best_grid_search.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.74        50\n",
      "           1       0.78      0.85      0.81        61\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       111\n",
      "   macro avg       0.79      0.78      0.78       111\n",
      "weighted avg       0.78      0.78      0.78       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 6 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    3.3s finished\n",
      "/home/gsus/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'clf__C': (0.001, 0.0015, 0.01, 1, 10, 100), 'clf__loss': ['squared_hinge'], 'clf__max_iter': [1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "                    ])\n",
    "\n",
    "parameters = {\n",
    "              'clf__C': (0.001, 0.0015, 0.01, 1, 10, 100),\n",
    "              'clf__loss': ['squared_hinge'],\n",
    "              'clf__max_iter': [1000],\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring='roc_auc',\n",
    "                           verbose=3,\n",
    "                           n_jobs = -1,\n",
    "                           cv=20                          \n",
    "                          )\n",
    "\n",
    "                       \n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1, 'clf__loss': 'squared_hinge', 'clf__max_iter': 1000}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid_search = grid_search.best_estimator_\n",
    "best_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 12]\n",
      " [16 45]]\n"
     ]
    }
   ],
   "source": [
    "# Form a prediction set\n",
    "predictions = best_grid_search.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73        50\n",
      "           1       0.79      0.74      0.76        61\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       111\n",
      "   macro avg       0.75      0.75      0.75       111\n",
      "weighted avg       0.75      0.75      0.75       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
